{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contextualized_topic_models\n",
      "  Downloading contextualized_topic_models-1.4.3-py2.py3-none-any.whl (21 kB)\n",
      "Collecting torch==1.6.0\n",
      "  Downloading torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |################################| 748.8 MB 13 kB/s s eta 0:00:01    |                                | 15.5 MB 10.6 MB/s eta 0:01:10     |###                             | 80.9 MB 59.8 MB/s eta 0:00:12     |#########                       | 229.3 MB 74.9 MB/s eta 0:00:07[K     |##########################      | 617.5 MB 62.7 MB/s eta 0:00:03[K     |##############################  | 709.4 MB 55.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: gensim==3.8.3 in /usr/local/lib/python3.6/dist-packages (from contextualized_topic_models) (3.8.3)\n",
      "Collecting torchvision==0.7.0\n",
      "  Downloading torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9 MB)\n",
      "\u001b[K     |################################| 5.9 MB 65.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest-runner==5.1\n",
      "  Downloading pytest_runner-5.1-py2.py3-none-any.whl (6.6 kB)\n",
      "Collecting numpy==1.19.1\n",
      "  Downloading numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5 MB)\n",
      "\u001b[K     |################################| 14.5 MB 73.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytest==4.6.5\n",
      "  Downloading pytest-4.6.5-py2.py3-none-any.whl (230 kB)\n",
      "\u001b[K     |################################| 230 kB 49.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel==0.33.6\n",
      "  Downloading wheel-0.33.6-py2.py3-none-any.whl (21 kB)\n",
      "Collecting sentence-transformers==0.3.5.1\n",
      "  Downloading sentence-transformers-0.3.5.1.tar.gz (61 kB)\n",
      "\u001b[K     |################################| 61 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->contextualized_topic_models) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (2.1.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim==3.8.3->contextualized_topic_models) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==0.7.0->contextualized_topic_models) (7.2.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0; python_version > \"2.7\" in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (8.5.0)\n",
      "Collecting atomicwrites>=1.0\n",
      "  Downloading atomicwrites-1.4.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (1.9.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12 in /usr/local/lib/python3.6/dist-packages (from pytest==4.6.5->contextualized_topic_models) (1.5.0)\n",
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[K     |################################| 769 kB 50.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.5.1->contextualized_topic_models) (4.44.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers==0.3.5.1->contextualized_topic_models) (0.23.2)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |################################| 1.4 MB 68.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.12.33)\n",
      "Requirement already satisfied, skipping upgrade: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->pytest==4.6.5->contextualized_topic_models) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.12->pytest==4.6.5->contextualized_topic_models) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers==0.3.5.1->contextualized_topic_models) (0.1.85)\n",
      "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers==0.3.5.1->contextualized_topic_models) (0.0.38)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |################################| 3.0 MB 55.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers==0.3.5.1->contextualized_topic_models) (2020.2.20)\n",
      "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers==0.3.5.1->contextualized_topic_models) (3.0.12)\n",
      "Requirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.2->sentence-transformers==0.3.5.1->contextualized_topic_models) (0.7)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.5.1->contextualized_topic_models) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers==0.3.5.1->contextualized_topic_models) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers==0.3.5.1->contextualized_topic_models) (7.0)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/lib/python3/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2020.6.20)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.9.5)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.33 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (1.15.33)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (0.15.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.33->boto3->smart-open>=1.8.1->gensim==3.8.3->contextualized_topic_models) (2.8.1)\n",
      "Building wheels for collected packages: sentence-transformers, nltk\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.5.1-py3-none-any.whl size=105431 sha256=7e659942b9253618b0d2275fe86b81e3e2f9e5bd95ec822f9ab64f9b39fc1ebd\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/54/bf/99d1804cefc0ea274e0a9d2eac930088a4c006cc7ae1deb0c8\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1435510 sha256=2dd361b3ad6a537da46aeafb55fa245846f94b8112c07acd7231da21658197d5\n",
      "  Stored in directory: /root/.cache/pip/wheels/de/5e/42/64abaeca668161c3e2cecc24f864a8fc421e3d07a104fc8a51\n",
      "Successfully built sentence-transformers nltk\n",
      "\u001b[31mERROR: ktrain 0.21.2 has requirement transformers>=3.1.0, but you'll have transformers 3.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: flair 0.6.0.post1 has requirement pytest>=5.3.2, but you'll have pytest 4.6.5 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, torch, torchvision, pytest-runner, atomicwrites, pytest, wheel, tokenizers, transformers, nltk, sentence-transformers, contextualized-topic-models\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0\n",
      "    Uninstalling torch-1.4.0:\n",
      "      Successfully uninstalled torch-1.4.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 6.0.1\n",
      "    Uninstalling pytest-6.0.1:\n",
      "      Successfully uninstalled pytest-6.0.1\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.30.0\n",
      "    Uninstalling wheel-0.30.0:\n",
      "      Successfully uninstalled wheel-0.30.0\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc2\n",
      "    Uninstalling tokenizers-0.8.1rc2:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.1.0\n",
      "    Uninstalling transformers-3.1.0:\n",
      "      Successfully uninstalled transformers-3.1.0\n",
      "Successfully installed atomicwrites-1.4.0 contextualized-topic-models-1.4.3 nltk-3.5 numpy-1.19.1 pytest-4.6.5 pytest-runner-5.1 sentence-transformers-0.3.5.1 tokenizers-0.8.1rc1 torch-1.6.0 torchvision-0.7.0 transformers-3.0.2 wheel-0.33.6\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U contextualized_topic_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create text\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('./data/eda.csv')\n",
    "texts  = df.tweet.values\n",
    "with open('./data/raw_tweet.txt', 'w', encoding=\"ascii\") as f: \n",
    "    for a in texts: \n",
    "        raw_i = a.encode('ascii', 'ignore')\n",
    "        f.writelines(f'{raw_i}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: \n",
      "               N Components: 50\n",
      "               Topic Prior Mean: 0.0\n",
      "               Topic Prior Variance: 0.98\n",
      "               Model Type: prodLDA\n",
      "               Hidden Sizes: (100, 100)\n",
      "               Activation: softplus\n",
      "               Dropout: 0.2\n",
      "               Learn Priors: True\n",
      "               Learning Rate: 0.002\n",
      "               Momentum: 0.99\n",
      "               Reduce On Plateau: False\n",
      "               Save Dir: None\n",
      "Epoch: [1/100]\tSamples: [3356/335600]\tTrain Loss: 239.8574998603248\tTime: 0:00:00.891933\n",
      "Epoch: [2/100]\tSamples: [6712/335600]\tTrain Loss: 232.37290414439624\tTime: 0:00:00.918237\n",
      "Epoch: [3/100]\tSamples: [10068/335600]\tTrain Loss: 222.88794294500522\tTime: 0:00:00.973752\n",
      "Epoch: [4/100]\tSamples: [13424/335600]\tTrain Loss: 215.14535838912582\tTime: 0:00:00.954467\n",
      "Epoch: [5/100]\tSamples: [16780/335600]\tTrain Loss: 210.01868039518772\tTime: 0:00:00.966576\n",
      "Epoch: [6/100]\tSamples: [20136/335600]\tTrain Loss: 205.2933367068776\tTime: 0:00:00.949920\n",
      "Epoch: [7/100]\tSamples: [23492/335600]\tTrain Loss: 201.9304656073078\tTime: 0:00:00.965051\n",
      "Epoch: [8/100]\tSamples: [26848/335600]\tTrain Loss: 198.97732954968714\tTime: 0:00:00.975947\n",
      "Epoch: [9/100]\tSamples: [30204/335600]\tTrain Loss: 196.60315188747765\tTime: 0:00:00.919060\n",
      "Epoch: [10/100]\tSamples: [33560/335600]\tTrain Loss: 195.00030524852875\tTime: 0:00:00.929844\n",
      "Epoch: [11/100]\tSamples: [36916/335600]\tTrain Loss: 194.01334742113008\tTime: 0:00:00.896391\n",
      "Epoch: [12/100]\tSamples: [40272/335600]\tTrain Loss: 193.0257518891072\tTime: 0:00:00.913587\n",
      "Epoch: [13/100]\tSamples: [43628/335600]\tTrain Loss: 191.89787850817567\tTime: 0:00:00.910289\n",
      "Epoch: [14/100]\tSamples: [46984/335600]\tTrain Loss: 191.71316997541717\tTime: 0:00:00.922333\n",
      "Epoch: [15/100]\tSamples: [50340/335600]\tTrain Loss: 190.90439988546632\tTime: 0:00:00.927703\n",
      "Epoch: [16/100]\tSamples: [53696/335600]\tTrain Loss: 189.97662651780394\tTime: 0:00:00.967911\n",
      "Epoch: [17/100]\tSamples: [57052/335600]\tTrain Loss: 189.6976907614161\tTime: 0:00:00.901742\n",
      "Epoch: [18/100]\tSamples: [60408/335600]\tTrain Loss: 189.20419788019592\tTime: 0:00:00.916531\n",
      "Epoch: [19/100]\tSamples: [63764/335600]\tTrain Loss: 189.35182380903606\tTime: 0:00:00.947293\n",
      "Epoch: [20/100]\tSamples: [67120/335600]\tTrain Loss: 188.47850660430944\tTime: 0:00:00.922826\n",
      "Epoch: [21/100]\tSamples: [70476/335600]\tTrain Loss: 188.35563417782515\tTime: 0:00:00.959091\n",
      "Epoch: [22/100]\tSamples: [73832/335600]\tTrain Loss: 188.1245782099691\tTime: 0:00:00.962912\n",
      "Epoch: [23/100]\tSamples: [77188/335600]\tTrain Loss: 187.45155685479367\tTime: 0:00:00.951549\n",
      "Epoch: [24/100]\tSamples: [80544/335600]\tTrain Loss: 187.2055160649769\tTime: 0:00:00.930851\n",
      "Epoch: [25/100]\tSamples: [83900/335600]\tTrain Loss: 187.25450801735698\tTime: 0:00:00.933893\n",
      "Epoch: [26/100]\tSamples: [87256/335600]\tTrain Loss: 186.81819103727466\tTime: 0:00:00.925101\n",
      "Epoch: [27/100]\tSamples: [90612/335600]\tTrain Loss: 186.47469696299538\tTime: 0:00:00.930281\n",
      "Epoch: [28/100]\tSamples: [93968/335600]\tTrain Loss: 186.17053731888782\tTime: 0:00:00.965983\n",
      "Epoch: [29/100]\tSamples: [97324/335600]\tTrain Loss: 185.88774812718825\tTime: 0:00:00.946570\n",
      "Epoch: [30/100]\tSamples: [100680/335600]\tTrain Loss: 186.01845575089393\tTime: 0:00:00.915955\n",
      "Epoch: [31/100]\tSamples: [104036/335600]\tTrain Loss: 185.43430594806875\tTime: 0:00:00.916916\n",
      "Epoch: [32/100]\tSamples: [107392/335600]\tTrain Loss: 185.1285529881183\tTime: 0:00:00.936018\n",
      "Epoch: [33/100]\tSamples: [110748/335600]\tTrain Loss: 184.80805134460667\tTime: 0:00:00.933776\n",
      "Epoch: [34/100]\tSamples: [114104/335600]\tTrain Loss: 184.8117742708954\tTime: 0:00:00.931424\n",
      "Epoch: [35/100]\tSamples: [117460/335600]\tTrain Loss: 184.63639051325984\tTime: 0:00:00.925002\n",
      "Epoch: [36/100]\tSamples: [120816/335600]\tTrain Loss: 184.57369584093786\tTime: 0:00:00.909280\n",
      "Epoch: [37/100]\tSamples: [124172/335600]\tTrain Loss: 184.10964067388633\tTime: 0:00:00.952966\n",
      "Epoch: [38/100]\tSamples: [127528/335600]\tTrain Loss: 184.39729565516984\tTime: 0:00:00.935557\n",
      "Epoch: [39/100]\tSamples: [130884/335600]\tTrain Loss: 183.8162240902488\tTime: 0:00:00.934979\n",
      "Epoch: [40/100]\tSamples: [134240/335600]\tTrain Loss: 183.46395317505028\tTime: 0:00:00.954194\n",
      "Epoch: [41/100]\tSamples: [137596/335600]\tTrain Loss: 183.647558710146\tTime: 0:00:00.918895\n",
      "Epoch: [42/100]\tSamples: [140952/335600]\tTrain Loss: 183.4575454293616\tTime: 0:00:00.927010\n",
      "Epoch: [43/100]\tSamples: [144308/335600]\tTrain Loss: 182.7723355208023\tTime: 0:00:00.922856\n",
      "Epoch: [44/100]\tSamples: [147664/335600]\tTrain Loss: 182.8630403102652\tTime: 0:00:00.904931\n",
      "Epoch: [45/100]\tSamples: [151020/335600]\tTrain Loss: 182.82708732959625\tTime: 0:00:00.913500\n",
      "Epoch: [46/100]\tSamples: [154376/335600]\tTrain Loss: 182.1371891353639\tTime: 0:00:00.921029\n",
      "Epoch: [47/100]\tSamples: [157732/335600]\tTrain Loss: 182.3143652460146\tTime: 0:00:00.975980\n",
      "Epoch: [48/100]\tSamples: [161088/335600]\tTrain Loss: 182.5009572116638\tTime: 0:00:00.901765\n",
      "Epoch: [49/100]\tSamples: [164444/335600]\tTrain Loss: 182.0683675227205\tTime: 0:00:00.912073\n",
      "Epoch: [50/100]\tSamples: [167800/335600]\tTrain Loss: 181.88818126582984\tTime: 0:00:00.905828\n",
      "Epoch: [51/100]\tSamples: [171156/335600]\tTrain Loss: 181.73576767822556\tTime: 0:00:00.914881\n",
      "Epoch: [52/100]\tSamples: [174512/335600]\tTrain Loss: 181.8443945428896\tTime: 0:00:00.932301\n",
      "Epoch: [53/100]\tSamples: [177868/335600]\tTrain Loss: 181.32528551940555\tTime: 0:00:00.978718\n",
      "Epoch: [54/100]\tSamples: [181224/335600]\tTrain Loss: 181.5036571625447\tTime: 0:00:00.917486\n",
      "Epoch: [55/100]\tSamples: [184580/335600]\tTrain Loss: 181.4732034567286\tTime: 0:00:00.920953\n",
      "Epoch: [56/100]\tSamples: [187936/335600]\tTrain Loss: 181.27686710835073\tTime: 0:00:00.932483\n",
      "Epoch: [57/100]\tSamples: [191292/335600]\tTrain Loss: 180.79904724048347\tTime: 0:00:00.932766\n",
      "Epoch: [58/100]\tSamples: [194648/335600]\tTrain Loss: 181.05581406785421\tTime: 0:00:00.946107\n",
      "Epoch: [59/100]\tSamples: [198004/335600]\tTrain Loss: 180.78074164043878\tTime: 0:00:00.911073\n",
      "Epoch: [60/100]\tSamples: [201360/335600]\tTrain Loss: 180.86202257267766\tTime: 0:00:00.934637\n",
      "Epoch: [61/100]\tSamples: [204716/335600]\tTrain Loss: 180.13564543331904\tTime: 0:00:00.916488\n",
      "Epoch: [62/100]\tSamples: [208072/335600]\tTrain Loss: 180.2706460327585\tTime: 0:00:00.900319\n",
      "Epoch: [63/100]\tSamples: [211428/335600]\tTrain Loss: 180.08874264377235\tTime: 0:00:00.959907\n",
      "Epoch: [64/100]\tSamples: [214784/335600]\tTrain Loss: 180.27552782679342\tTime: 0:00:00.950896\n",
      "Epoch: [65/100]\tSamples: [218140/335600]\tTrain Loss: 179.8220018425488\tTime: 0:00:00.913344\n",
      "Epoch: [66/100]\tSamples: [221496/335600]\tTrain Loss: 180.10968199446887\tTime: 0:00:00.906467\n",
      "Epoch: [67/100]\tSamples: [224852/335600]\tTrain Loss: 180.38789987848256\tTime: 0:00:00.912923\n",
      "Epoch: [68/100]\tSamples: [228208/335600]\tTrain Loss: 179.65612691122243\tTime: 0:00:00.972358\n",
      "Epoch: [69/100]\tSamples: [231564/335600]\tTrain Loss: 179.92697576402338\tTime: 0:00:00.935840\n",
      "Epoch: [70/100]\tSamples: [234920/335600]\tTrain Loss: 179.30835397422527\tTime: 0:00:00.937748\n",
      "Epoch: [71/100]\tSamples: [238276/335600]\tTrain Loss: 179.62845419700722\tTime: 0:00:00.912967\n",
      "Epoch: [72/100]\tSamples: [241632/335600]\tTrain Loss: 179.31218674924574\tTime: 0:00:00.915020\n",
      "Epoch: [73/100]\tSamples: [244988/335600]\tTrain Loss: 179.46394109896454\tTime: 0:00:00.945697\n",
      "Epoch: [74/100]\tSamples: [248344/335600]\tTrain Loss: 179.85070116954708\tTime: 0:00:00.917478\n",
      "Epoch: [75/100]\tSamples: [251700/335600]\tTrain Loss: 178.5207816341068\tTime: 0:00:00.906887\n",
      "Epoch: [76/100]\tSamples: [255056/335600]\tTrain Loss: 179.07242828841999\tTime: 0:00:00.938253\n",
      "Epoch: [77/100]\tSamples: [258412/335600]\tTrain Loss: 178.69246050683478\tTime: 0:00:00.963016\n",
      "Epoch: [78/100]\tSamples: [261768/335600]\tTrain Loss: 178.97746631499552\tTime: 0:00:00.963524\n",
      "Epoch: [79/100]\tSamples: [265124/335600]\tTrain Loss: 178.72139568124254\tTime: 0:00:00.920604\n",
      "Epoch: [80/100]\tSamples: [268480/335600]\tTrain Loss: 178.9132664968061\tTime: 0:00:00.913471\n",
      "Epoch: [81/100]\tSamples: [271836/335600]\tTrain Loss: 178.44199666874627\tTime: 0:00:00.907829\n",
      "Epoch: [82/100]\tSamples: [275192/335600]\tTrain Loss: 178.40266893134498\tTime: 0:00:00.938851\n",
      "Epoch: [83/100]\tSamples: [278548/335600]\tTrain Loss: 178.5349039616545\tTime: 0:00:00.938942\n",
      "Epoch: [84/100]\tSamples: [281904/335600]\tTrain Loss: 178.14945887496276\tTime: 0:00:00.907380\n",
      "Epoch: [85/100]\tSamples: [285260/335600]\tTrain Loss: 177.8458157381369\tTime: 0:00:00.949647\n",
      "Epoch: [86/100]\tSamples: [288616/335600]\tTrain Loss: 177.54340072468153\tTime: 0:00:00.916487\n",
      "Epoch: [87/100]\tSamples: [291972/335600]\tTrain Loss: 178.05087582175582\tTime: 0:00:00.878368\n",
      "Epoch: [88/100]\tSamples: [295328/335600]\tTrain Loss: 178.06381789379097\tTime: 0:00:00.970316\n",
      "Epoch: [89/100]\tSamples: [298684/335600]\tTrain Loss: 177.71796636388186\tTime: 0:00:00.966444\n",
      "Epoch: [90/100]\tSamples: [302040/335600]\tTrain Loss: 177.79916945629097\tTime: 0:00:01.001139\n",
      "Epoch: [91/100]\tSamples: [305396/335600]\tTrain Loss: 177.4389418555386\tTime: 0:00:00.969669\n",
      "Epoch: [92/100]\tSamples: [308752/335600]\tTrain Loss: 177.4012761076244\tTime: 0:00:00.910716\n",
      "Epoch: [93/100]\tSamples: [312108/335600]\tTrain Loss: 177.4394886257822\tTime: 0:00:00.914864\n",
      "Epoch: [94/100]\tSamples: [315464/335600]\tTrain Loss: 177.40879034285607\tTime: 0:00:00.958818\n",
      "Epoch: [95/100]\tSamples: [318820/335600]\tTrain Loss: 177.44150053076578\tTime: 0:00:00.914101\n",
      "Epoch: [96/100]\tSamples: [322176/335600]\tTrain Loss: 177.17440064786018\tTime: 0:00:00.907748\n",
      "Epoch: [97/100]\tSamples: [325532/335600]\tTrain Loss: 177.6512548653531\tTime: 0:00:00.924461\n",
      "Epoch: [98/100]\tSamples: [328888/335600]\tTrain Loss: 177.34609916241433\tTime: 0:00:00.936787\n",
      "Epoch: [99/100]\tSamples: [332244/335600]\tTrain Loss: 176.8948340251136\tTime: 0:00:00.923994\n",
      "Epoch: [100/100]\tSamples: [335600/335600]\tTrain Loss: 177.0290934729775\tTime: 0:00:00.911639\n"
     ]
    }
   ],
   "source": [
    "from contextualized_topic_models.models.ctm import CTM\n",
    "from contextualized_topic_models.utils.data_preparation import TextHandler\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file\n",
    "from contextualized_topic_models.datasets.dataset import CTMDataset\n",
    "\n",
    "handler = TextHandler(\"./data/raw_tweet.txt\")\n",
    "handler.prepare() # create vocabulary and training data\n",
    "\n",
    "training_bert = bert_embeddings_from_file(\"./data/raw_tweet.txt\", \"distiluse-base-multilingual-cased\")\n",
    "\n",
    "training_dataset = CTMDataset(handler.bow, training_bert, handler.idx2token)\n",
    "\n",
    "ctm = CTM(input_size=len(handler.vocab), bert_input_size=512, inference_type=\"contextual\", n_components=50)\n",
    "\n",
    "ctm.fit(training_dataset) # run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "topics = ctm.get_topic_lists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ga saya apa nya ya? kurang apakah hari kalo ada\n",
      "1 Zulfikar! menghargai dipertimbangkan mendatang apresiasinya. kak' butuhkan Indra KoinWorks-nya. merek\n",
      "2 KoinWorks. 50.000 I Lets investment worth b'Hey, give Detail join!\n",
      "3 harus pandemi untuk salah seperti lakukan yang membuat sangat karyawan,\n",
      "4 THR Jaga Bersama Berbuka Puasa Media: Hati, b'KoinWorks http://rviv.ly/ezCJbw' http://kabarsuka.com/?p=1440&utm_source=ReviveOldPost&utm_medium=social&utm_campaign=ReviveOldPost'\n",
      "5 ini karena saat yaa maaf Tapi tapi beberapa ada sekarang\n",
      "6 Anda ' akun ke yaa tim kami atas saat Mohon\n",
      "7 to 18% one up in Get you First me single\n",
      "8 atas ke dulu cek kami alamat DM email diinfokan b'Hai\n",
      "9 dicek Yuk balas Boleh DM DM. Anda. yaa, kirim Message\n",
      "10 The #fintech - of Platform : financial raises b'Indonesias Capital\n",
      "11 ke atas cek DM dulu bantu alamat Maaf infokan email\n",
      "12 . for $75 be https://www.audible.com/pd/B078WHKMN3/?source_code=AUDFPWS0223189MWT-BK-ACX0-105123&ref=acx_bty_BK_ACX0_105123_rh_us' every days consecutively) bounty(have 91\n",
      "13 b'Sudah Sudah dibalas b'Siap, ya' b'Okay b'Iya, Bambang! b'DM sama-sama\n",
      "14 b'Iya kak' Indra butuhkan KoinWorks-nya. merek b'Okee model \\n\\nApakah masukan,\n",
      "15 KOIN and click is by 350,000* or supervised get code\n",
      "16 KOIN Financial get code \\n or my and 350,000* click\n",
      "17 pinjaman aplikasi online KoinWorks dan layanan pada peminjam keuangan dalam\n",
      "18 Dan Saldo kamu KoinRobo Langsung hasil Gabung Saat Peer A93068\n",
      "19 aku aja tahu b'Aku udah makin punya invest tau pake\n",
      "20 dengan dari usaha tanggal mendapatkan nomor terbaik OJK gede lebih\n",
      "21 Investasi b'Halo, Ayo Gabung! Detil https://koinworks.com/invite/a96793' kasih Lebih Baik Crowde,\n",
      "22 Bisa dana di ingin tentang COVID-19 investasi sekarang! #koinworks mengubungi\n",
      "23 Koinworks sebagai bekerja Investree, P2P finansial lending Modalku, produk terbaru\n",
      "24 to one Get me up First introduce 18% let App.\n",
      "25 buat sampaikan mau turuturut semuanya sampai turut DENGAN videonya SISCA\n",
      "26 Dana b'Terima Manajemen @koinworks' Mandiri Test Kit Kerja Reksa Talenta\n",
      "27 itu yg kalau aja aku bunga udah mulai langsung kalo\n",
      "28 I Lets worth investment b'Hey, give Detail join! KoinWorks. #Indonesia\n",
      "29 b'@koinworks , ya? saya min, buka login kok kenapa sy\n",
      "30 UANG MANA SAYA SAYA' MEI SDH KENTUT.... BILANG JGN TP\n",
      "31 butuhkan nominalnya dikenakan KoinWorks-nya. merek b'Okee model begini, tahunan). (jika\n",
      "32 MiKo melalui tidak dong kamu minta nyaman. bisa pertanyaan lebih\n",
      "33 Rp.100.000.- besaran Zulfikar! menghargai dipertimbangkan mendatang apresiasinya. apresiasinya Indra Diza!\n",
      "34 b'Koinworks Miliar Perlindungan Rp #Fintech' Dapat Covid-19 Pandemi Prudential Rupiah\n",
      "35 for . be $75 a https://www.audible.com/pd/B078WHKMN3/?source_code=AUDFPWS0223189MWT-BK-ACX0-105123&ref=acx_bty_BK_ACX0_105123_rh_us' days every consecutively) bounty(have\n",
      "36 platform telah oleh tahun menjadi 2019, hingga pelaku berhasil kepada\n",
      "37 from bags | Lendable $10m : funding b'Indonesias raises x\n",
      "38 b'@koinworks ya?' bank berapa login koinworks b'Min min voucher min,\n",
      "39 on b'Check Audible out https://www.audible.com/pd/Elven-Magic-Book-1-2-3-and-4-Audiobook/B078WHKMN3?qid=1578536018&sr=1-1&pf_rd_p=e81b7c27-6880-467a-b5a7-13cef5d729fe&pf_rd_r=8HSZQ1NAT7REM0S4K01V&ref=a_search_c3_lProduct_1_1' Elven Book Audio Magic .\n",
      "40 Untuk klik email data detail your b'Hi our > screenshot\n",
      "41 maintenis ama b'Min ya?' b'koinworks blom mah maintenance didapatkan pemerintah.\n",
      "42 kita @koinworks banyak bikin nggak biar billboard selalu tetap mereka\n",
      "43 tanpa semua App digital beli KoinGold 1 pertama Kamu Super\n",
      "44 investasi\\nNabung https://koinworks.onelink.me/LgVI/7f23e8ea' Sosialisasi https://ift.tt/39o8cm1' https://bandungberita.com/kembali-buktikan-dukungannya-terhadap-digital-ukm-indonesia-koinworks-kantongi-10-juta-usd/#.XsIpAveySEc Kecanggihan Mengenalkan Kenalkan https://ift.tt/2wIo5FA' 15%\n",
      "45 saya 1 yg ada invest tgl koinworks, ga belum portfolio\n",
      "46 for on . out $75 b'Check be https://www.audible.com/pd/B078WHKMN3/?source_code=AUDFPWS0223189MWT-BK-ACX0-105123&ref=acx_bty_BK_ACX0_105123_rh_us' Elven days\n",
      "47 kami atas ' boleh Anda mohon kendala yaa Maaf b'Hi\n",
      "48 sampaikan sampai turuturut terus semuanya mau turut kit test videonya\n",
      "49 the of your and app is check new by financial\n"
     ]
    }
   ],
   "source": [
    "for i,t in enumerate(topics): \n",
    "    print(i, ' '.join(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
